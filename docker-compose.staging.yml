# Staging configuration for DocuTag
# Assumes external reverse proxy (e.g., Caddy, Nginx, Traefik) handles:
# - SSL/TLS termination
# - Routing docutag.honker subdomain to web container
# - Load balancing if needed
#
# External proxy should route:
#   docutag.honker -> localhost:3001 (Web UI)
#
# The web container's nginx then proxies /api/* to the controller service internally

services:
  controller:
    image: ghcr.io/docutag/docutag-controller:staging
    container_name: docutag-controller
    labels:
      app: docutag
      app.type: backend
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      # Service URLs (internal Docker network)
      - SCRAPER_BASE_URL=http://scraper:8080
      - TEXTANALYZER_BASE_URL=http://textanalyzer:8080
      - SCHEDULER_BASE_URL=http://scheduler:8080

      # External URLs
      - WEB_INTERFACE_URL=https://docutag.honker
      - PUBLIC_URL_BASE=https://docutag.honker

      # Database
      - DATABASE_PATH=/app/data/controller.db

      # Disable mock data in staging
      - GENERATE_MOCK_DATA=false

      # CORS disabled (same-origin requests via nginx proxy)
      - CORS_ORIGINS=

      # Observability
      - TEMPO_ENDPOINT=tempo:4317

      # Queue configuration
      - REDIS_ADDR=redis:6379
      - WORKER_CONCURRENCY=10
      - MAX_LINK_DEPTH=1

      # Port
      - CONTROLLER_PORT=8080
    ports:
      - "9080:8080"  # Expose for external reverse proxy
    volumes:
      - controller-data:/app/data
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
      scraper:
        condition: service_healthy
      textanalyzer:
        condition: service_healthy
      loki:
        condition: service_started
      tempo:
        condition: service_started
    networks:
      - docutag-network
      - proxy

  scraper:
    image: ghcr.io/docutag/docutag-scraper:staging
    container_name: docutag-scraper
    labels:
      app: docutag
      app.type: backend
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      # Ollama configuration
      - USE_OLLAMA=true
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=gemma3:12b

      # Database & storage
      - DATABASE_PATH=/app/data/scraper.db
      - STORAGE_PATH=/app/storage

      # Observability
      - TEMPO_ENDPOINT=tempo:4317

      # Port
      - SCRAPER_PORT=8080
    ports:
      - "9081:8080"  # Optional: expose if you need direct access for debugging
    volumes:
      - scraper-data:/app/data
      - scraper-storage:/app/storage
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    restart: unless-stopped
    depends_on:
      loki:
        condition: service_started
      tempo:
        condition: service_started
    networks:
      - docutag-network
      - proxy

  textanalyzer:
    image: ghcr.io/docutag/docutag-textanalyzer:staging
    container_name: docutag-textanalyzer
    labels:
      app: docutag
      app.type: backend
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      # Ollama configuration
      - USE_OLLAMA=true
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=gemma3:12b

      # Database
      - DATABASE_PATH=/data/textanalyzer.db

      # Queue configuration
      - REDIS_ADDR=redis:6379
      - WORKER_CONCURRENCY=5

      # Observability
      - TEMPO_ENDPOINT=tempo:4317

      # Port
      - TEXTANALYZER_PORT=8080
    ports:
      - "9082:8080"  # Optional: expose if you need direct access for debugging
    volumes:
      - textanalyzer-data:/data
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
      loki:
        condition: service_started
      tempo:
        condition: service_started
    networks:
      - docutag-network
      - proxy

  scheduler:
    image: ghcr.io/docutag/docutag-scheduler:staging
    container_name: docutag-scheduler
    labels:
      app: docutag
      app.type: backend
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      # Controller URL (internal)
      - CONTROLLER_BASE_URL=http://controller:8080

      # Scraper URL
      - SCRAPER_URL=http://scraper:8080

      # Database
      - DATABASE_PATH=/app/data/scheduler.db
      - CONTROLLER_DB_PATH=/app/controller-data/controller.db

      # Observability
      - TEMPO_ENDPOINT=tempo:4317

      # Port
      - SCHEDULER_PORT=8080

      # Scheduling configuration
      - ENABLE_SCHEDULER=true
      - DEFAULT_SCHEDULE=0 */6 * * *  # Every 6 hours for staging
    ports:
      - "9083:8080"  # Optional: expose if you need direct access for debugging
    volumes:
      - scheduler-data:/app/data
      - controller-data:/app/controller-data:ro
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s
    restart: unless-stopped
    depends_on:
      controller:
        condition: service_healthy
      scraper:
        condition: service_healthy
      loki:
        condition: service_started
      tempo:
        condition: service_started
    networks:
      - docutag-network

  # Redis - Message queue backend
  redis:
    image: redis:7-alpine
    container_name: docutag-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --save 60 1
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s
    networks:
      - docutag-network

  # Asynqmon - Queue monitoring UI
  asynqmon:
    image: hibiken/asynqmon:latest
    container_name: docutag-asynqmon
    platform: linux/amd64  # Force x86_64 for ARM64 compatibility
    ports:
      - "9084:8080"
    environment:
      - REDIS_ADDR=redis:6379
    command: ["--enable-metrics-exporter", "--prometheus-addr=http://prometheus:9090"]
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
      prometheus:
        condition: service_started
    networks:
      - docutag-network
      - proxy

  web:
    image: ghcr.io/docutag/docutag-web:staging
    container_name: docutag-web
    labels:
      app: docutag
      app.type: frontend
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      # Runtime nginx config (if needed)
      - NGINX_PORT=80
    ports:
      - "3001:80"  # Expose for external reverse proxy
    restart: unless-stopped
    depends_on:
      controller:
        condition: service_healthy
    networks:
      - docutag-network
      - proxy

  # Loki - Log aggregation
  loki:
    image: grafana/loki:3.0.0
    container_name: docutag-loki
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - loki-data:/loki
      - ./config/loki-config.yaml:/etc/loki/local-config.yaml
    restart: unless-stopped
    networks:
      - docutag-network

  # Promtail - Log collector for Loki
  promtail:
    image: grafana/promtail:3.0.0
    container_name: docutag-promtail
    volumes:
      - ./config/promtail:/etc/promtail
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/promtail-config.yaml
    restart: unless-stopped
    depends_on:
      - loki
    networks:
      - docutag-network

  # Tempo - Distributed tracing
  tempo:
    image: grafana/tempo:latest
    container_name: docutag-tempo
    command: [ "-config.file=/etc/tempo.yaml" ]
    ports:
      - "3200:3200"   # Tempo
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    volumes:
      - tempo-data:/var/tempo
      - ./config/tempo-config.yaml:/etc/tempo.yaml
    restart: unless-stopped
    networks:
      - docutag-network

  # Grafana - Observability dashboard
  grafana:
    image: grafana/grafana:11.0.0
    container_name: docutag-grafana
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    ports:
      - "3000"
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SECURITY_ADMIN_USER=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=https://docutag.honker/grafana
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_SERVER_LANDING_PAGE=/grafana/dashboards
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped
    depends_on:
      - loki
      - tempo
    networks:
      - docutag-network
      - proxy

  # Prometheus - Metrics collection and monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: docutag-prometheus
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    ports:
      - "9091:9090"  # Use 9091 on host to avoid conflicts with local dev
    volumes:
      - ./config/prometheus:/etc/prometheus
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - docutag-network
    depends_on:
      - loki

  # Node Exporter - System and Docker volume metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: docutag-node-exporter
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped
    networks:
      - docutag-network

# Networks and volumes for staging
networks:
  docutag-network:
    driver: bridge
  proxy:
    external: true

volumes:
  controller-data:
    driver: local
  scraper-data:
    driver: local
  scraper-storage:
    driver: local
  textanalyzer-data:
    driver: local
  scheduler-data:
    driver: local
  redis-data:
    driver: local
  loki-data:
    driver: local
  tempo-data:
    driver: local
  grafana-data:
    driver: local
  prometheus-data:
    driver: local
